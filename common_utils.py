"""Common helpers & shared singletons for all LangGraph workflows.

Place this file at the root of your project (or package it as a module) and
`import` the utilities you need.  This keeps each workflow light-weight and
ensures you only spin up one LLM / search client per process.
"""

from __future__ import annotations

from dotenv import load_dotenv

load_dotenv()  

from typing import Annotated
from typing_extensions import TypedDict

from langgraph.graph.message import add_messages
from langchain.chat_models import init_chat_model
from langchain_tavily import TavilySearch
from langchain.schema import BaseMessage

# ---------------------------------------------------------------------------
# ðŸ—ï¸  Typed-state base class
# ---------------------------------------------------------------------------

class BaseState(TypedDict, total=False):
    """Minimal state definition: only the conversational message list.

    Inæ¯å€‹ workflow æƒ³å†æ“´å……åˆ¥çš„æ¬„ä½ï¼ˆä¾‹å¦‚ search_queryã€approvedâ‹¯ï¼‰ï¼Œ
    ç›´æŽ¥ç¹¼æ‰¿ `BaseState` å†åŠ æ¬„ä½å³å¯ï¼š

        class MyState(BaseState):
            search_query: str
            ...
    """

    messages: Annotated[list, add_messages]


# ---------------------------------------------------------------------------
# ðŸ”§  Utility helpers
# ---------------------------------------------------------------------------

def msg_content(msg: BaseMessage | dict) -> str:
    """Return the text content from either a LangChain `BaseMessage` object
    or a raw `{"role": ..., "content": ...}` dict.
    """
    return msg["content"] if isinstance(msg, dict) else msg.content


# ---------------------------------------------------------------------------
# ðŸ¤–  Shared singletons (LLM & Search tool)
# ---------------------------------------------------------------------------

llm = init_chat_model("google_genai:gemini-2.0-flash")
search_tool = TavilySearch(max_results=10)
